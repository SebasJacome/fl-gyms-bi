{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnZFDFueQTk-"
   },
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnvKMEBZQaT5"
   },
   "outputs": [],
   "source": [
    "lf_business = pl.scan_parquet(\"./data/business.parquet\")\n",
    "lf_review   = pl.scan_parquet(\"./data/review.parquet\")\n",
    "lf_tip      = pl.scan_parquet(\"./data/tip.parquet\")\n",
    "lf_user     = pl.scan_parquet(\"./data/user.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXL3o-6ZQbke"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"./data/outputs_nlp\"\n",
    "\n",
    "CUSTOM_STOPWORDS = {\n",
    "    \"ve\", \"dont\", \"didnt\", \"doesnt\", \"im\", \"ive\", \"youre\", \"didn\", \"doesn\",\n",
    "    \"cant\", \"couldnt\", \"wouldnt\", \"shouldnt\", \"wont\", \"isnt\", \"arent\", \"wasnt\", \"werent\",\n",
    "    \"theyre\", \"weve\", \"yelp\", \"gym\", \"class\", \"classes\"\n",
    "}\n",
    "\n",
    "_clean_url = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "_clean_non_letters = re.compile(r\"[^a-zA-Z\\s']\")\n",
    "_clean_multi_space = re.compile(r\"\\s+\")\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    s = _clean_url.sub(\" \", s)\n",
    "    s = _clean_non_letters.sub(\" \", s)\n",
    "    s = _clean_multi_space.sub(\" \", s).strip()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mn1THM9CQeFB"
   },
   "outputs": [],
   "source": [
    "reviews_min = (\n",
    "    lf_review\n",
    "      .select([\"stars\", \"text\"])\n",
    "      .drop_nulls()\n",
    ")\n",
    "\n",
    "bad_texts = (\n",
    "    reviews_min\n",
    "      .filter(pl.col(\"stars\").is_in([1, 2]))\n",
    "      .select(pl.col(\"text\").cast(pl.Utf8))\n",
    "      .collect()\n",
    "      .get_column(\"text\")\n",
    "      .to_list()\n",
    ")\n",
    "\n",
    "good_texts = (\n",
    "    reviews_min\n",
    "      .filter(pl.col(\"stars\").is_in([4, 5]))\n",
    "      .select(pl.col(\"text\").cast(pl.Utf8))\n",
    "      .collect()\n",
    "      .get_column(\"text\")\n",
    "      .to_list()\n",
    ")\n",
    "\n",
    "len_bad, len_good = len(bad_texts), len(good_texts)\n",
    "print(f\"Total reseñas malas: {len_bad} | Total reseñas buenas: {len_good}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFYnUGbTQfdk"
   },
   "outputs": [],
   "source": [
    "\n",
    "bad_texts_clean  = [normalize_text(t) for t in bad_texts if isinstance(t, str)]\n",
    "good_texts_clean = [normalize_text(t) for t in good_texts if isinstance(t, str)]\n",
    "\n",
    "bad_texts_clean  = [t for t in bad_texts_clean  if t]\n",
    "good_texts_clean = [t for t in good_texts_clean if t]\n",
    "\n",
    "print(f\"Tras limpieza -> malas: {len(bad_texts_clean)} | buenas: {len(good_texts_clean)}\")\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import text as sklearn_text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def top_ngrams(\n",
    "    texts,\n",
    "    ngram_range=(1,1),\n",
    "    top_k=30,\n",
    "    min_df=5,\n",
    "    max_df=0.9,\n",
    "    extra_stopwords=None\n",
    "):\n",
    "    if not texts:\n",
    "        return []\n",
    "    if extra_stopwords is None:\n",
    "        extra_stopwords = set()\n",
    "    stop_words = list(sklearn_text.ENGLISH_STOP_WORDS.union(CUSTOM_STOPWORDS).union(extra_stopwords))\n",
    "\n",
    "    n_docs = len(texts)\n",
    "    safe_min_df = min_df\n",
    "    if isinstance(min_df, int) and min_df > n_docs:\n",
    "        safe_min_df = 1\n",
    "\n",
    "    vect = CountVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        stop_words=stop_words,\n",
    "        min_df=safe_min_df,\n",
    "        max_df=max_df\n",
    "    )\n",
    "    try:\n",
    "        X = vect.fit_transform(texts)\n",
    "    except ValueError as e:\n",
    "        print(f\"[Aviso] Vocabulario vacío: {e}\")\n",
    "        return []\n",
    "    if X.shape[1] == 0:\n",
    "        return []\n",
    "\n",
    "    freqs = X.sum(axis=0).A1\n",
    "    vocab = vect.get_feature_names_out()\n",
    "    pairs = list(zip(vocab, freqs))\n",
    "    pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    return pairs[:top_k]\n",
    "\n",
    "\n",
    "TOP_K   = 30\n",
    "MIN_DF  = 5\n",
    "MAX_DF  = 0.9\n",
    "EXTRA_SW = set()\n",
    "\n",
    "bad_uni = top_ngrams(bad_texts_clean,  (1,1), top_k=TOP_K, min_df=MIN_DF, max_df=MAX_DF, extra_stopwords=EXTRA_SW)\n",
    "bad_bi  = top_ngrams(bad_texts_clean,  (2,2), top_k=TOP_K, min_df=MIN_DF, max_df=MAX_DF, extra_stopwords=EXTRA_SW)\n",
    "bad_tri = top_ngrams(bad_texts_clean,  (3,3), top_k=TOP_K, min_df=MIN_DF, max_df=MAX_DF, extra_stopwords=EXTRA_SW)\n",
    "\n",
    "good_uni = top_ngrams(good_texts_clean, (1,1), top_k=TOP_K, min_df=MIN_DF, max_df=MAX_DF, extra_stopwords=EXTRA_SW)\n",
    "good_bi  = top_ngrams(good_texts_clean, (2,2), top_k=TOP_K, min_df=MIN_DF, max_df=MAX_DF, extra_stopwords=EXTRA_SW)\n",
    "good_tri = top_ngrams(good_texts_clean, (3,3), top_k=TOP_K, min_df=MIN_DF, max_df=MAX_DF, extra_stopwords=EXTRA_SW)\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "NOISE_UNI = {\n",
    "    \"great\",\"good\",\"best\",\"love\",\"really\",\"just\",\"like\",\n",
    "    \"time\",\"people\",\"place\",\"area\",\"location\",\"work\",\"going\",\"said\",\"told\",\"know\",\"don\",\n",
    "    \"month\",\"months\",\"member\",\"fitness\"\n",
    "}\n",
    "NOISE_BI = {\n",
    "    \"la fitness\",\"planet fitness\",\"anytime fitness\",\"great place\",\"st pete\",\"love place\",\n",
    "    \"feel like\",\"make sure\",\"member years\",\"month month\"\n",
    "}\n",
    "NOISE_TRI = {\n",
    "    \"tampa bay area\",\"anytime fitness locations\",\"hyde park location\",\"south tampa location\",\n",
    "    \"great place work\",\"great place workout\",\"make feel like\",\"say good things\",\n",
    "    \"la fitness locations\",\"member la fitness\"\n",
    "}\n",
    "NOISE_PATTERNS = [\n",
    "    re.compile(r\"\\b.* location\\b\"),\n",
    "    re.compile(r\"\\b(st|saint)\\b\"),\n",
    "]\n",
    "\n",
    "def is_noise(term: str, n: int) -> bool:\n",
    "    t = term.lower().strip()\n",
    "    if n == 1 and t in NOISE_UNI: return True\n",
    "    if n == 2 and t in NOISE_BI:  return True\n",
    "    if n == 3 and t in NOISE_TRI: return True\n",
    "    for rx in NOISE_PATTERNS:\n",
    "        if rx.search(t):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filter_pairs(pairs, n):\n",
    "    out = []\n",
    "    for term, freq in pairs:\n",
    "        if not term:\n",
    "            continue\n",
    "        if len(term.split()) != n:\n",
    "            continue\n",
    "        if is_noise(term, n):\n",
    "            continue\n",
    "        if any(len(tok) <= 2 for tok in term.split()):\n",
    "            continue\n",
    "        out.append((term, int(freq)))\n",
    "    return out\n",
    "\n",
    "bad_uni_f  = filter_pairs(bad_uni, 1)\n",
    "bad_bi_f   = filter_pairs(bad_bi,  2)\n",
    "bad_tri_f  = filter_pairs(bad_tri, 3)\n",
    "\n",
    "good_uni_f = filter_pairs(good_uni, 1)\n",
    "good_bi_f  = filter_pairs(good_bi,  2)\n",
    "good_tri_f = filter_pairs(good_tri, 3)\n",
    "\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "WC_DIR = OUTPUT_DIR\n",
    "\n",
    "\n",
    "def pairs_to_freq(pairs, join_char=\"_\"):\n",
    "    freqs = {}\n",
    "    for term, freq in pairs:\n",
    "        t = str(term).strip().replace(\" \", join_char)\n",
    "        freqs[t] = freqs.get(t, 0) + int(freq)\n",
    "    return freqs\n",
    "\n",
    "def merge_freqs(*weighted_pairs, join_char=\"_\"):\n",
    "    freqs = {}\n",
    "    for pairs, w in weighted_pairs:\n",
    "        for term, freq in pairs:\n",
    "            t = str(term).strip().replace(\" \", join_char)\n",
    "            val = int(freq) * float(w)\n",
    "            freqs[t] = freqs.get(t, 0.0) + val\n",
    "    return freqs\n",
    "\n",
    "def plot_wc(freqs, title, filename=None, width=1600, height=800):\n",
    "    if not freqs:\n",
    "        print(f\"[WordCloud] Sin datos: {title}\")\n",
    "        return\n",
    "    wc = WordCloud(\n",
    "        width=width, height=height, background_color=\"white\",\n",
    "        collocations=False, normalize_plurals=False\n",
    "    ).generate_from_frequencies(freqs)\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if filename:\n",
    "        out = WC_DIR + '/' + filename\n",
    "        wc.to_file(str(out))\n",
    "        print(\"Guardado:\", out)\n",
    "    plt.show()\n",
    "\n",
    "TOP_N_CLOUD = 200\n",
    "def top_n_freq(d, n=200):\n",
    "    return dict(sorted(d.items(), key=lambda kv: kv[1], reverse=True)[:n])\n",
    "\n",
    "bad_all_freq  = merge_freqs((bad_uni_f, 1.0), (bad_bi_f, 1.25), (bad_tri_f, 1.5))\n",
    "good_all_freq = merge_freqs((good_uni_f, 1.0), (good_bi_f, 1.25), (good_tri_f, 1.5))\n",
    "\n",
    "plot_wc(top_n_freq(bad_all_freq,  TOP_N_CLOUD),  \"Reseñas MALAS — Uni+Bi+Tri (limpios)\",  \"wc_bad_all.png\")\n",
    "plot_wc(top_n_freq(good_all_freq, TOP_N_CLOUD), \"Reseñas BUENAS — Uni+Bi+Tri (limpios)\", \"wc_good_all.png\")\n",
    "\n",
    "plot_wc(pairs_to_freq(bad_uni_f),  \"MALAS — Unigramas (limpios)\",  \"wc_bad_uni.png\")\n",
    "plot_wc(pairs_to_freq(bad_bi_f),   \"MALAS — Bigramas (limpios)\",   \"wc_bad_bi.png\")\n",
    "plot_wc(pairs_to_freq(bad_tri_f),  \"MALAS — Trigramas (limpios)\",  \"wc_bad_tri.png\")\n",
    "\n",
    "plot_wc(pairs_to_freq(good_uni_f), \"BUENAS — Unigramas (limpios)\", \"wc_good_uni.png\")\n",
    "plot_wc(pairs_to_freq(good_bi_f),  \"BUENAS — Bigramas (limpios)\",  \"wc_good_bi.png\")\n",
    "plot_wc(pairs_to_freq(good_tri_f), \"BUENAS — Trigramas (limpios)\", \"wc_good_tri.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
